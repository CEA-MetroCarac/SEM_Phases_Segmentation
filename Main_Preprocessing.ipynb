{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6289c339-75d2-4f3f-b5d3-0d4a88ef1d36",
   "metadata": {},
   "source": [
    "# Load the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de89bbc1-7e42-43b0-997a-9b154ab77a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import cv2\n",
    "# import pandas as pd\n",
    "\n",
    "import skimage.io as ski\n",
    "import os\n",
    "# from sklearn.datasets import load_sample_image\n",
    "# from sklearn.feature_extraction import image\n",
    "# from patchify import patchify\n",
    "import tifffile as tiff\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from Patches_lib import generate_patches, Image_augmentation\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# from keras import layers\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e31f7-04f0-40ac-ba10-39a5329cefd9",
   "metadata": {},
   "source": [
    "# Load images and create and save patches\n",
    "\n",
    "The large images of size 2856×4096 with the same acquisition conditions are pre-processed and divided into smaller patches of $256 \\times 256$ pixels.  Randomly selected patches were used for testing purposes later.\n",
    "\n",
    "<img src=\"Figure/Patchings.svg\"/>\n",
    "\n",
    "## Load images\n",
    "\n",
    "- `patch_size`  \n",
    "  This defines the size (in pixels) of square patches that the input images are divided into.  \n",
    "  For example, if `patch_size` is $256$, each image will be processed in blocks of $256 \\times 256$ pixels.  \n",
    "  This is commonly used in patch-based training for deep learning models, particularly in image segmentation tasks.\n",
    "\n",
    "- `Number_testing_images`  \n",
    "  This indicates the total number of images used for testing the model's performance.  \n",
    "  These images are not used during training or validation and are reserved exclusively to evaluate generalization.\n",
    "\n",
    "- `Number_augmented_images`  \n",
    "  This specifies the number of augmented versions generated per original image.  \n",
    "  Data augmentation helps improve model robustness by artificially increasing the training dataset through transformations such as rotations, flips, or brightness changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a8b800-187f-4e37-9387-e29da5502f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Images have been detected and loaded.\n"
     ]
    }
   ],
   "source": [
    "# Stock images and labels name:\n",
    "Data_folder = 'Data'\n",
    "patch_size = 256\n",
    "Number_testing_images   = 20\n",
    "Number_augmented_images = 10\n",
    "\n",
    "All_Images_names = sorted(list(Path(Data_folder + '/SEM Image/').glob('*.tif')))\n",
    "All_Labels_names = sorted(list(Path(Data_folder + '/Labels/').glob('*.tif')))\n",
    "\n",
    "print(len(All_Images_names), 'Images have been detected and loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad209c9-2a1d-4071-b321-7087ec5e08f1",
   "metadata": {},
   "source": [
    "## Create and save patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e24dd99-f601-4fb6-8f4c-4e1b05c38472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Image 1: Data/SEM Image/Image_0.tif\n",
      "Directory created: Data/Training_Images_256\n",
      "Directory created: Data/Training_Labels_256\n",
      "Directory created: Data/Testing_Images_256\n",
      "Directory created: Data/Testing_Labels_256\n",
      "Patches correctly saved\n",
      "Processing Image 2: Data/SEM Image/Image_1.tif\n",
      "Directory already exists: Data/Training_Images_256\n",
      "Directory already exists: Data/Training_Labels_256\n",
      "Directory already exists: Data/Testing_Images_256\n",
      "Directory already exists: Data/Testing_Labels_256\n",
      "Patches correctly saved\n",
      "Processing Image 3: Data/SEM Image/Image_2.tif\n",
      "Directory already exists: Data/Training_Images_256\n",
      "Directory already exists: Data/Training_Labels_256\n",
      "Directory already exists: Data/Testing_Images_256\n",
      "Directory already exists: Data/Testing_Labels_256\n",
      "Patches correctly saved\n",
      "Processing Image 4: Data/SEM Image/Image_3.tif\n",
      "Directory already exists: Data/Training_Images_256\n",
      "Directory already exists: Data/Training_Labels_256\n",
      "Directory already exists: Data/Testing_Images_256\n",
      "Directory already exists: Data/Testing_Labels_256\n",
      "Patches correctly saved\n",
      "Processing Image 5: Data/SEM Image/Image_4.tif\n",
      "Directory already exists: Data/Training_Images_256\n",
      "Directory already exists: Data/Training_Labels_256\n",
      "Directory already exists: Data/Testing_Images_256\n",
      "Directory already exists: Data/Testing_Labels_256\n",
      "Patches correctly saved\n",
      "Processing Image 6: Data/SEM Image/Image_5.tif\n",
      "Directory already exists: Data/Training_Images_256\n",
      "Directory already exists: Data/Training_Labels_256\n",
      "Directory already exists: Data/Testing_Images_256\n",
      "Directory already exists: Data/Testing_Labels_256\n",
      "Patches correctly saved\n",
      "Processing Image 7: Data/SEM Image/Image_6.tif\n",
      "Directory already exists: Data/Training_Images_256\n",
      "Directory already exists: Data/Training_Labels_256\n",
      "Directory already exists: Data/Testing_Images_256\n",
      "Directory already exists: Data/Testing_Labels_256\n",
      "Patches correctly saved\n"
     ]
    }
   ],
   "source": [
    "# Necessary functions to create and save patches\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Directory created: {path}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {path}\")\n",
    "\n",
    "def save_patch(image, label, index, image_id, patch_type, patch_size, base_dir):\n",
    "    image_path = os.path.join(base_dir, f\"{patch_type}_Images_{patch_size}\", f\"Image_{image_id}_{index}.tif\")\n",
    "    label_path = os.path.join(base_dir, f\"{patch_type}_Labels_{patch_size}\", f\"Image_{image_id}_{index}.tif\")\n",
    "    tiff.imwrite(image_path, image)\n",
    "    tiff.imwrite(label_path, label)\n",
    "\n",
    "def prepare_data(images, labels, patch_size, base_dir, save_it, num_test=20):\n",
    "    for image_id, (img_path, lbl_path) in enumerate(zip(images, labels)):\n",
    "\n",
    "        print(f\"Processing Image {image_id + 1}: {img_path}\")\n",
    "\n",
    "        # Read image and label\n",
    "        image = ski.imread(img_path)\n",
    "        label = ski.imread(lbl_path)\n",
    "\n",
    "        # Generate patches (assumes this function is defined elsewhere)\n",
    "        patches_img, patches_lbl = generate_patches(image, label, patch_size, Type='Seq')\n",
    "\n",
    "        # Normalize label\n",
    "        patches_lbl = (patches_lbl / 85).astype(np.uint8)\n",
    "\n",
    "        # Define output directories\n",
    "        for folder in ['Training_Images_', 'Training_Labels_', 'Testing_Images_', 'Testing_Labels_']:\n",
    "            create_directory(os.path.join(base_dir, f\"{folder}{patch_size}\"))\n",
    "\n",
    "        num_patches = patches_img.shape[0]\n",
    "        test_indices = np.random.choice(num_patches, num_test, replace=False)\n",
    "\n",
    "        for idx in range(num_patches):\n",
    "            patch_type = \"Testing\" if idx in test_indices else \"Training\"\n",
    "            if save_it != False:\n",
    "                save_patch(patches_img[idx], patches_lbl[idx], idx, image_id, patch_type, patch_size, base_dir)\n",
    "        if save_it != False: \n",
    "            print('Patches correctly saved')\n",
    "        else:\n",
    "            print('Patches are not saved')\n",
    "\n",
    "# Load the functions\n",
    "prepare_data(All_Images_names, All_Labels_names, patch_size=patch_size, base_dir= Data_folder, save_it= True, num_test=Number_testing_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11254153-c645-4e17-95d0-2b184fa23ff9",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "<img src=\"Figure/Augmentation.svg\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19400bbc-5238-40e3-9025-22c1326b2cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created: Data/Augmented_Images_256\n",
      "Directory created: Data/Augmented_Labels_256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data Augmentation:   0%|                                                                      | 0/10920 [00:00<?, ?it/s]/nobackup/kd264511/envs/new_env/lib/python3.11/site-packages/pydantic/main.py:212: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "/nobackup/kd264511/envs/new_env/lib/python3.11/site-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "Data Augmentation: 100%|██████████████████████████████████████████████████████████| 10920/10920 [04:13<00:00, 43.08it/s]\n"
     ]
    }
   ],
   "source": [
    "def save_image_and_label(image, label, index, aug_index, image_dir, label_dir):\n",
    "    \"\"\"Save image and label pair with a specific naming format.\"\"\"\n",
    "    image_path = os.path.join(image_dir, f\"Image_{index}_{aug_index}.tif\")\n",
    "    label_path = os.path.join(label_dir, f\"Image_{index}_{aug_index}.tif\")\n",
    "    tiff.imwrite(image_path, image)\n",
    "    tiff.imwrite(label_path, label)\n",
    "\n",
    "def augment_dataset(data_dir, patch_size, augment_fn, total_images_per_sample=10):\n",
    "    image_dir = Path(data_dir) / f'Training_Images_{patch_size}'\n",
    "    label_dir = Path(data_dir) / f'Training_Labels_{patch_size}'\n",
    "    \n",
    "    all_images = sorted(image_dir.glob('*.tif'))\n",
    "    all_labels = sorted(label_dir.glob('*.tif'))\n",
    "\n",
    "    # Only generate (total_images_per_sample - 1) augmentations since original is included\n",
    "    num_augmentations = total_images_per_sample - 1\n",
    "    total_samples = len(all_images) * total_images_per_sample\n",
    "\n",
    "    aug_image_dir = Path(data_dir) / f'Augmented_Images_{patch_size}'\n",
    "    aug_label_dir = Path(data_dir) / f'Augmented_Labels_{patch_size}'\n",
    "\n",
    "    create_directory(aug_image_dir)\n",
    "    create_directory(aug_label_dir)\n",
    "\n",
    "    with tqdm(total=total_samples, desc='Data Augmentation') as pbar:\n",
    "        for idx, (img_path, lbl_path) in enumerate(zip(all_images, all_labels)):\n",
    "            image = ski.imread(img_path)\n",
    "            label = ski.imread(lbl_path)\n",
    "\n",
    "            # Save original image and label\n",
    "            save_image_and_label(image, label, idx, 0, aug_image_dir, aug_label_dir)\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Generate n-1 augmentations\n",
    "            aug_images, aug_labels = augment_fn(image, [image, label], num_transformations=num_augmentations)\n",
    "\n",
    "            for aug_idx in range(num_augmentations):\n",
    "                save_image_and_label(aug_images[aug_idx], aug_labels[aug_idx], idx, aug_idx + 1, aug_image_dir, aug_label_dir)\n",
    "                pbar.update(1)\n",
    "                \n",
    "\n",
    "augment_dataset(data_dir=Data_folder, patch_size=patch_size, augment_fn=Image_augmentation, total_images_per_sample=Number_augmented_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
